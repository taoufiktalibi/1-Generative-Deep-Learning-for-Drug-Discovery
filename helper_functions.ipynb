{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0cd0cb84-83ba-4a4e-ab9f-9ae68bc8e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "27f549dd-182b-4d6a-ae28-ed3cc30b82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"c:\\\\users\\\\talibit\\\\Drug_discovery_data\\\\data.txt\"\n",
    "DATA_PATH1 = \"c:\\\\users\\\\talibit\\\\Drug_discovery_data\\\\alzheimersdata.txt\"\n",
    "data = open(DATA_PATH, 'r').read().splitlines()\n",
    "data1 = open(DATA_PATH1, 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ac93fad-87f1-436f-b57e-e4d7a2f197bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "17dc4608-e349-4cbd-b19e-2f49380927ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "count =0\n",
    "for molecule in data:\n",
    "    count+=1\n",
    "    if 30 < len(molecule) < 50 and count < 300000:\n",
    "        if ('T' not in molecule) and ('V' not in molecule) and ('g' not in molecule) and ('L' not in molecule) and ('8' not in molecule):\n",
    "            new.append(molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a8139a06-6d8b-4bbb-a23f-5cbc47e2c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = []\n",
    "count =0\n",
    "for molecule in data1:\n",
    "    count+=1\n",
    "    if 30 < len(molecule) < 50 and count < 300000:\n",
    "        if ('T' not in molecule) and ('V' not in molecule) and ('g' not in molecule) and ('L' not in molecule) and ('8' not in molecule):\n",
    "            new1.append(molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4dbf4c9d-624f-427f-ab5c-c4d1906c7610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b4908b0c-39a8-4b3f-b23f-6c6ed4e67713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = len(max(new, key=len))\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8338b765-d6d5-406a-8aa2-3bfb5e537f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len1 = len(max(new1, key=len))\n",
    "max_seq_len1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "06caf998-bf8a-441c-9b34-21417eb64ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [['$']+ list(i) for i in new]\n",
    "padded = pad_sequences(l, maxlen=max_seq_len+1, padding=\"post\", value=\"£\",dtype=object) \n",
    "padded_text =[\"\".join(i) for i in padded]\n",
    "datas = np.array(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8998625-cc62-4aee-ae7b-436812a087f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [['$']+ list(i) for i in new1]\n",
    "padded1 = pad_sequences(l1, maxlen=max_seq_len1+1, padding=\"post\", value=\"£\",dtype=object) \n",
    "padded_text1 =[\"\".join(i) for i in padded1]\n",
    "datas1 = np.array(padded_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "770b94ea-e6c2-466f-85a8-26bf4cf89b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datas1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eacd05a3-10b6-4d60-95cf-9f25f659f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training validation and test sets (generic data)\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "full_train, test = train_test_split(np.array(datas), test_size=0.2, random_state=seed)\n",
    "\n",
    "train, val = train_test_split(np.array(full_train), test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "564c6a90-3f21-4d01-a65b-9a1c165f5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training validation and test sets (alzheimer data)\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "full_train1, test1 = train_test_split(np.array(datas1), test_size=0.2, random_state=seed)\n",
    "\n",
    "train1, val1 = train_test_split(np.array(full_train1), test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6bad9d61-7f58-4225-b5f0-e1f2e8edbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ''.join(train) \n",
    "val = ''.join(val) \n",
    "# so now train and val are a long string where each chemical string starts with $ and ends with £..£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ca823268-3999-4596-a081-744be9f77114",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = ''.join(train1) \n",
    "val1 = ''.join(val1) \n",
    "# so now train and val are a long string where each chemical string starts with $ and ends with £..£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e7f2ff4e-45d8-4b88-a922-7da914ad8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function return for a sequence of chars a tuple of (sequence,next char) where each sequence is a window of seqLen characters\n",
    "# and the next char is the one following the last char of the sequence\n",
    "#the sequences are built by moving through the whole sequence by stepSize each time\n",
    "def n_grams(seqLen, stepSize, data):\n",
    "\n",
    "    input_chars = [] \n",
    "    next_char = [] \n",
    "    for i in range(0, len(data) - seqLen, stepSize): \n",
    "        input_chars.append(data[i : i + seqLen]) \n",
    "        next_char.append(data[i + seqLen]) \n",
    "\n",
    "    return input_chars, next_char\n",
    "\n",
    "# the following function prepare data to feed the neural net\n",
    "def preparation(data, tokenizer):\n",
    "    tokenizer.fit_on_texts(data) \n",
    "    new_data = tokenizer.texts_to_sequences(data) \n",
    "\n",
    "    #N-Grams Sequence \n",
    "    seqLen = 15 \n",
    "    stepSize = 1 \n",
    "    input_chars, next_char = n_grams(seqLen, stepSize, new_data)\n",
    "  \n",
    "    #Assemble Validation Datasets \n",
    "    x_data = np.array(input_chars) \n",
    "    #x_data.flatten() \n",
    "    y_data = np.array(next_char) \n",
    "    #y_data_2 = np_utils.to_categorical(y_data) \n",
    "    return x_data, y_data\n",
    "\n",
    "def generate_curves(history, filename):\n",
    "    fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
    "\n",
    "    axis1.plot(history.history[\"accuracy\"], label='Train', linewidth=3)\n",
    "    axis1.plot(history.history[\"val_accuracy\"], label='Validation', linewidth=3)\n",
    "    axis1.set_title('Model accuracy', fontsize=16, color=\"white\")\n",
    "    axis1.set_ylabel('accuracy')\n",
    "    axis1.set_xlabel('epoch')\n",
    "    axis1.legend(loc='lower right')\n",
    "\n",
    "    axis2.plot(history.history[\"loss\"], label='Train', linewidth=3)\n",
    "    axis2.plot(history.history[\"val_loss\"], label='Validation', linewidth=3)\n",
    "    axis2.set_title('Model loss', fontsize=16, color=\"white\")\n",
    "    axis2.set_ylabel('loss')\n",
    "    axis2.set_xlabel('epoch')\n",
    "    axis2.legend(loc='upper right')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e9228b84-6e6b-4bc9-82a3-7e9ac3717c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheking the n_grams function\n",
    "tokenizer = Tokenizer(num_words=None, char_level=True, lower=False) \n",
    "\n",
    "tokenizer.fit_on_texts(train) \n",
    "new_data = tokenizer.texts_to_sequences(train) \n",
    "\n",
    "\n",
    "#N-Grams Sequence \n",
    "seqLen = 15 \n",
    "stepSize = 1 \n",
    "input_chars, next_char = n_grams(seqLen, stepSize, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "506c5519-3978-410b-972f-95c8bc4376ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheking the n_grams function\n",
    "tokenizer1 = Tokenizer(num_words=None, char_level=True, lower=False) \n",
    "\n",
    "tokenizer1.fit_on_texts(train1) \n",
    "new_data1 = tokenizer.texts_to_sequences(train1) \n",
    "\n",
    "\n",
    "#N-Grams Sequence \n",
    "seqLen1 = 15 \n",
    "stepSize = 1 \n",
    "input_chars, next_char = n_grams(seqLen1, stepSize, new_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f5c01c32-77bb-4c61-94a1-b40260f6b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preparation(train, tokenizer)\n",
    "\n",
    "x_val, y_val = preparation(val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "607e5f08-abc9-415f-b642-7684b407df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, y_train1 = preparation(train1, tokenizer1)\n",
    "\n",
    "x_val1, y_val1 = preparation(val1, tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "39c35bbd-867c-4d37-bb9e-fc16a7492a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7796fcd5-b5a8-46c4-bd8d-f068d9fcbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size1 = len(tokenizer1.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "03bd7e0b-90a4-4772-a8ec-277ecafc6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing test data for alzheimer dataset\n",
    "test2 = ''.join(test1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e2689635-6a17-434e-87d3-4ef4b82390a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = preparation(test2, tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "535de436-0547-419e-ad4a-c64ba985c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(seed,tokenizer,num_char,model):\n",
    "    for i in range(num_char):\n",
    "        tokens = tokenizer.texts_to_sequences([seed])[0]\n",
    "        tokens = np.array(tokens)\n",
    "        tokens = tokens[i:15+i]\n",
    "        tokens = tokens.reshape(1,15)\n",
    "        predicted = model.predict(tokens) #shape (1,43)\n",
    "        ind = np.argmax(predicted)\n",
    "    \n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            predicted_word = \"\"\n",
    "            if index == ind:\n",
    "                predicted_word = word\n",
    "                break\n",
    "            if predicted_word == \"£\":\n",
    "                break\n",
    "\n",
    "        seed += predicted_word\n",
    "    return seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f4a42384-c851-499b-8536-0725a8b5bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSMILES(smiles):\n",
    "\tvalid_smiles = []\n",
    "\tfor smile in smiles:\n",
    "\t\tnew = ''\n",
    "\t\tfor char in smile:\n",
    "\t\t\tif char != '$' and char != '£':\t\n",
    "\t\t\t\tnew += char\n",
    "\n",
    "\t\tif Chem.MolFromSmiles(new, sanitize=False) is not None and len(new) > 15:\n",
    "\t\t\tvalid_smiles.append(new)\n",
    "\t\t\tprint(new)\n",
    "\n",
    "\treturn valid_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2991bfa-fda6-426b-adef-e14fd8f7c935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
